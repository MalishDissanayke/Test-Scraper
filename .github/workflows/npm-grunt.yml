name: Scrape Data and Push

on:
  schedule:
    - cron: '*/5 * * * *'  # Run every 5 minutes
  workflow_dispatch:  # Allows manual triggering

jobs:
  scrape-and-push-and-commit:
    runs-on: ubuntu-latest  # Running on Ubuntu in GitHub CI

    steps:
      # Step 1: Checkout the code
      - name: Checkout code
        uses: actions/checkout@v2

      # Step 2: Set up Node.js
      - name: Set up Node.js
        uses: actions/setup-node@v2
        with:
          node-version: '16'

      # Step 3: Install dependencies
      - name: Install dependencies
        run: npm install

      # Step 4: Install Google Chrome and necessary dependencies
      - name: Install dependencies for Puppeteer
        run: |
          sudo apt update
          sudo apt install -y wget gnupg2
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
          sudo apt update
          sudo apt install -y google-chrome-stable

      # Step 5: Run the scraper
      - name: Run the scraper and update files
        run: |
          export PUPPETEER_EXECUTABLE_PATH="/usr/bin/google-chrome-stable"
          node scraper.js

      # Step 6: Set up GitHub authentication (using the default GITHUB_TOKEN)
      - name: Set up GitHub authentication
        run: |
          git remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/MalishDissanayke/Test-Scraper.git

      # Step 7: Commit and push changes
      - name: Commit and push changes
        run: |
          git add prematch.json live.json
          git commit -m "Update scraped data" || echo "No changes to commit"
          git push
